{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUTORIAL: https://pyro.ai/examples/prodlda.html\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn #pip install torch torchvision torchaudio\n",
    "import torch.nn.functional as F\n",
    "import pyro #pip3 install pyro-ppl \n",
    "from pyro.infer import SVI, TraceMeanField_ELBO,  MCMC, NUTS \n",
    "import pyro.distributions as dist\n",
    "from tqdm import trange #pip install ipywidgets\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to execute prodLDAModel\n",
    "class Encoder(nn.Module):\n",
    "    # Base class for the encoder net, used in the guide\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fcmu = nn.Linear(hidden, num_topics)\n",
    "        self.fclv = nn.Linear(hidden, num_topics)\n",
    "        # NB: here we set `affine=False` to reduce the number of learning parameters\n",
    "        # See https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        # for the effect of this flag in BatchNorm1d\n",
    "        self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "        self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = F.softplus(self.fc1(inputs))\n",
    "        h = F.softplus(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        # Œº and Œ£ are the outputs\n",
    "        logtheta_loc = self.bnmu(self.fcmu(h))\n",
    "        logtheta_logvar = self.bnlv(self.fclv(h))\n",
    "        logtheta_scale = (0.5 * logtheta_logvar).exp()  # Enforces positivity\n",
    "        return logtheta_loc, logtheta_scale\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.drop(inputs)\n",
    "        # the output is œÉ(Œ≤Œ∏)\n",
    "        return F.softmax(self.bn(self.beta(inputs)), dim=1)\n",
    "\n",
    "\n",
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_topics = num_topics\n",
    "        self.encoder = Encoder(vocab_size, num_topics, hidden, dropout)\n",
    "        self.decoder = Decoder(vocab_size, num_topics, dropout)\n",
    "\n",
    "    def model(self, docs):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ùëù(ùúÉ|ùõº) is replaced by a logistic-normal distribution\n",
    "            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics))\n",
    "            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics))\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "            theta = F.softmax(logtheta, -1)\n",
    "\n",
    "            # conditional distribution of ùë§ùëõ is defined as\n",
    "            # ùë§ùëõ|ùõΩ,ùúÉ ~ Categorical(ùúé(ùõΩùúÉ))\n",
    "            count_param = self.decoder(theta)\n",
    "            # Currently, PyTorch Multinomial requires `total_count` to be homogeneous.\n",
    "            # Because the numbers of words across documents can vary,\n",
    "            # we will use the maximum count accross documents here.\n",
    "            # This does not affect the result because Multinomial.log_prob does\n",
    "            # not require `total_count` to evaluate the log probability.\n",
    "            total_count = int(docs.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs',\n",
    "                dist.Multinomial(total_count, count_param),\n",
    "                obs=docs\n",
    "            )\n",
    "\n",
    "    def guide(self, docs):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ùëù(ùúÉ|ùõº) is replaced by a logistic-normal distribution,\n",
    "            # where Œº and Œ£ are the encoder network outputs\n",
    "            logtheta_loc, logtheta_scale = self.encoder(docs)\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "\n",
    "    def beta(self):\n",
    "        # beta matrix elements are the weights of the FC layer on the decoder\n",
    "        return self.decoder.beta.weight.cpu().detach().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of all topics\n",
    "def get_topics(model, vocab, num_topics):\n",
    "    topics = []\n",
    "    for i in range(0, num_topics):\n",
    "        topic = model.beta()[i] #ottengo i pesi\n",
    "        sorted_, indices = torch.sort(topic, descending=True)\n",
    "        df = pd.DataFrame(indices.numpy(), columns=['index'])\n",
    "        words = pd.merge(df, vocab[['index', 'word']], how='left', on='index')['word'].values.tolist()\n",
    "        topics.append(words)\n",
    "\n",
    "    return topics\n",
    "\n",
    "def compute_coherence(model, n_topic, texts, vocab):\n",
    "    #List of all topics\n",
    "    topic_list = get_topics(model,vocab, n_topic)\n",
    "    dictionary = Dictionary(topic_list)\n",
    "    \n",
    "    #texts = list of lists of the documents (tweets)\n",
    "    #topics = list of lists of the topics\n",
    "    #dictionary = dizionario\n",
    "    coherence_cv = CoherenceModel(model = None, texts = texts,  topics = topic_list, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_umass = CoherenceModel(model = None, texts = texts,  topics = topic_list, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    coherence_cuci = CoherenceModel(model = None, texts = texts,  topics = topic_list, dictionary=dictionary, coherence='c_uci').get_coherence()\n",
    "    coherence_cnpmi = CoherenceModel(model = None, texts = texts,  topics = topic_list, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "    \n",
    "    return  coherence_umass,coherence_cv, coherence_cuci, coherence_cnpmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prodLDA(num_topics, batch_size, learning_rate, num_epochs, docs):\n",
    "    # training\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    prodLDA = ProdLDA(\n",
    "        vocab_size=docs.shape[1],\n",
    "        num_topics=num_topics,\n",
    "        hidden=100,\n",
    "        dropout=0.2\n",
    "    )\n",
    "    prodLDA.to(device)\n",
    "\n",
    "    optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "    svi = SVI(prodLDA.model, prodLDA.guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "    num_batches = int(math.ceil(docs.shape[0] / batch_size)) \n",
    "\n",
    "    bar = trange(num_epochs)\n",
    "    for epoch in bar:\n",
    "        running_loss = 0.0\n",
    "        for i in range(num_batches):\n",
    "            batch_docs = docs[i * batch_size:(i + 1) * batch_size, :]\n",
    "            loss = svi.step(batch_docs)\n",
    "            running_loss += loss / batch_docs.size(0)\n",
    "\n",
    "        bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))\n",
    "        \n",
    "    #Saving the model\n",
    "    path = f\"models_prodLDA/prodLDA_{num_topics}_{num_epochs}.pth\"\n",
    "    torch.save(prodLDA.state_dict(), path)\n",
    "    \n",
    "    #Show the topic  (10 words)\n",
    "    for i in range(0, num_topics):\n",
    "        topic = prodLDA.beta()[i] #ottengo i pesi\n",
    "        sorted_, indices = torch.sort(topic, descending=True)\n",
    "        df = pd.DataFrame(indices[:10].numpy(), columns=['index'])\n",
    "        words = pd.merge(df, vocab[['index', 'word']], how='left', on='index')['word'].values.tolist()\n",
    "        print(f\"topic{i+1} with {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataset to be used in the model\n",
    "source = '../doc/cleaned.csv'\n",
    "df = pd.read_csv(source)\n",
    "\n",
    "# Split the text data into words based on spaces\n",
    "tweets = df['lemmatized_text'].apply(lambda text: text.split())\n",
    "\n",
    "# Create a CountVectorizer \n",
    "# max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\n",
    "# min_df is used for removing terms that appear too infrequently, at least in 20 documents\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=20)\n",
    "\n",
    "# Convert the tokenized text data into a document-term matrix\n",
    "docs = torch.from_numpy(vectorizer.fit_transform([\" \".join(tweet) for tweet in tweets]).toarray())\n",
    "\n",
    "vocab = pd.DataFrame(columns=['word', 'index'])\n",
    "vocab['word'] = vectorizer.get_feature_names_out()\n",
    "vocab['index'] = vocab.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 5590\n",
      "Corpus size: torch.Size([70000, 5590])\n"
     ]
    }
   ],
   "source": [
    "#Show the initial data\n",
    "\n",
    "print('Dictionary size: %d' % len(vocab)) #vocab_size\n",
    "print('Corpus size: {}'.format(docs.shape)) # (num_docs, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting global variables\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "pyro.set_rng_seed(seed)\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\") # Uncomment this to run on GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload data on device\n",
    "docs = docs.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [32:41<00:00, 39.23s/it, epoch_loss=1.98e+05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic1 with ['biden', 'president', 'state', 'job', 'joe', 'infrastructure', 'business', 'hunter', 'federal', 'america']\n",
      "topic2 with ['independence', 'sent', 'july', 'playing', 'october', 'file', 'pres', 'earlier', 'ticket', 'involvement']\n",
      "topic3 with ['colorado', 'vaccine', 'tool', 'fire', 'emergency', 'area', 'road', 'read', 'california', 'accountable']\n",
      "topic4 with ['right', 'republican', 'woman', 'day', 'today', 'vote', 'house', 'freedom', 'abortion', 'democrat']\n",
      "topic5 with ['cost', 'american', 'care', 'family', 'act', 'health', 'tax', 'year', 'prescription', 'need']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Experiment\n",
    "run_prodLDA(num_topics=5, batch_size=32, learning_rate=1e-3, num_epochs=50, docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [33:44<00:00, 40.49s/it, epoch_loss=1.98e+05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic1 with ['buck', 'certain', 'collectively', 'manchin', 'windfall', 'treating', 'undermines', 'stunt', 'abusing', 'politicized']\n",
      "topic2 with ['trump', 'house', 'right', 'election', 'republican', 'abortion', 'vote', 'woman', 'gun', 'court']\n",
      "topic3 with ['biden', 'health', 'care', 'child', 'border', 'joe', 'million', 'veteran', 'access', 'hunter']\n",
      "topic4 with ['year', 'cost', 'inflation', 'insulin', 'price', 'drug', 'war', 'medicare', 'energy', 'act']\n",
      "topic5 with ['join', 'event', 'live', 'watch', 'honor', 'day', 'tomorrow', 'celebrate', 'tonight', 'loved']\n",
      "topic6 with ['people', 'work', 'working', 'country', 'job', 'worker', 'american', 'like', 'america', 'economy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Experiment\n",
    "run_prodLDA(num_topics=6, batch_size=32, learning_rate=1e-3, num_epochs=50, docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [32:28<00:00, 38.98s/it, epoch_loss=1.98e+05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic1 with ['discus', 'debate', 'week', 'join', 'watch', 'news', 'county', 'presidential', 'desantis', 'campaign']\n",
      "topic2 with ['lost', 'pact', 'shooting', 'story', 'officer', 'killed', 'police', 'one', 'service', 'loved']\n",
      "topic3 with ['people', 'country', 'america', 'work', 'like', 'nation', 'american', 'fight', 'world', 'know']\n",
      "topic4 with ['giant', 'normal', 'populist', 'authoritarian', 'withhold', 'divisive', 'direct', 'slogan', 'stuff', 'elite']\n",
      "topic5 with ['republican', 'vote', 'house', 'democrat', 'law', 'right', 'abortion', 'election', 'court', 'party']\n",
      "topic6 with ['border', 'child', 'health', 'care', 'crisis', 'help', 'need', 'funding', 'security', 'program']\n",
      "topic7 with ['biden', 'president', 'job', 'trump', 'cost', 'joe', 'price', 'economy', 'american', 'big']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Experiment\n",
    "run_prodLDA(num_topics=7, batch_size=32, learning_rate=1e-3, num_epochs=50, docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment\n",
    "#run_prodLDA(num_topics=8, batch_size=32, learning_rate=1e-3, num_epochs=50, docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [32:25<00:00, 38.91s/it, epoch_loss=1.99e+05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic1 with ['fox', 'apply', 'check', 'read', 'haley', 'interview', 'news', 'hit', 'deadline', 'learn']\n",
      "topic2 with ['join', 'water', 'town', 'live', 'discus', 'district', 'hall', 'team', 'looking', 'senator']\n",
      "topic3 with ['right', 'trump', 'election', 'abortion', 'vote', 'woman', 'candidate', 'voter', 'republican', 'reproductive']\n",
      "topic4 with ['day', 'today', 'life', 'black', 'honor', 'nation', 'celebrate', 'year', 'love', 'history']\n",
      "topic5 with ['family', 'health', 'cost', 'child', 'pay', 'care', 'student', 'million', 'tax', 'drug']\n",
      "topic6 with ['democrat', 'republican', 'house', 'energy', 'job', 'bill', 'act', 'party', 'gop', 'senate']\n",
      "topic7 with ['biden', 'president', 'gun', 'border', 'joe', 'hunter', 'administration', 'national', 'community', 'safety']\n",
      "topic8 with ['worker', 'america', 'business', 'work', 'support', 'like', 'working', 'world', 'china', 'economy']\n",
      "topic9 with ['meantime', 'withhold', 'repealing', 'tackling', 'smarter', 'enact', 'accept', 'overwhelmingly', 'promising', 'rewarded']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Experiment\n",
    "run_prodLDA(num_topics=9, batch_size=32, learning_rate=1e-3, num_epochs=50, docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [32:33<00:00, 39.06s/it, epoch_loss=2.00e+05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic1 with ['cost', 'tax', 'price', 'drug', 'insulin', 'prescription', 'government', 'spending', 'medicare', 'money']\n",
      "topic2 with ['supported', 'withhold', 'ohioan', 'meantime', 'illusion', 'outsider', 'percentage', 'implemented', 'lifting', 'elderly']\n",
      "topic3 with ['republican', 'right', 'trump', 'democrat', 'house', 'election', 'abortion', 'vote', 'state', 'gop']\n",
      "topic4 with ['gun', 'security', 'military', 'national', 'ukraine', 'violence', 'war', 'israel', 'weapon', 'stop']\n",
      "topic5 with ['health', 'care', 'school', 'veteran', 'child', 'access', 'student', 'need', 'benefit', 'like']\n",
      "topic6 with ['wishing', 'ohioan', 'meantime', 'wish', 'unified', 'outsider', 'remark', 'largely', 'punished', 'percentage']\n",
      "topic7 with ['day', 'today', 'year', 'life', 'black', 'honor', 'ago', 'woman', 'nation', 'remember']\n",
      "topic8 with ['border', 'energy', 'investment', 'clean', 'infrastructure', 'crisis', 'climate', 'funding', 'water', 'migrant']\n",
      "topic9 with ['biden', 'president', 'people', 'joe', 'american', 'economy', 'job', 'worker', 'trump', 'good']\n",
      "topic10 with ['week', 'join', 'discus', 'menendez', 'team', 'charge', 'investigation', 'special', 'local', 'meeting']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Experiment\n",
    "run_prodLDA(num_topics=10, batch_size=32, learning_rate=1e-3, num_epochs=50, docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5 topics: umass -6.442153045738394, cv 0.4885890115641576, cuci -2.889770153163904, cnpmi -0.07811261595651942\n",
      "\n",
      "Model 6 topics: umass -6.413647822049828, cv 0.5275921933368924, cuci -2.1169069550891573, cnpmi -0.0434046053669024\n",
      "\n",
      "Model 7 topics: umass -5.76732096299721, cv 0.545204573964516, cuci -1.59275476550026, cnpmi -0.020834793564301846\n",
      "\n",
      "Model 8 topics: umass -6.094559950416134, cv 0.5716934331399013, cuci -2.054536039432442, cnpmi -0.03250586457527792\n",
      "\n",
      "Model 9 topics: umass -5.961885053765611, cv 0.5103509865089653, cuci -1.809696119943097, cnpmi -0.028238792136889297\n",
      "\n",
      "Model 10 topics: umass -6.961382892863424, cv 0.5624399578927939, cuci -2.3985323530216536, cnpmi -0.03918545963545757\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#GET THE COHERENCE\n",
    "model5 = ProdLDA(vocab_size=docs.shape[1],num_topics=5,  hidden=100,dropout=0.2)\n",
    "model5.load_state_dict(torch.load(\"models_prodLDA/prodLDA_5_50.pth\"))\n",
    "umass,cv,cuci,cnpmi = compute_coherence(model5, 5, tweets, vocab)\n",
    "print(f\"Model 5 topics: umass {umass}, cv {cv}, cuci {cuci}, cnpmi {cnpmi}\\n\")\n",
    "\n",
    "model6 = ProdLDA(vocab_size=docs.shape[1],num_topics=6,  hidden=100,dropout=0.2)\n",
    "model6.load_state_dict(torch.load(\"models_prodLDA/prodLDA_6_50.pth\"))\n",
    "umass,cv,cuci,cnpmi = compute_coherence(model6, 6, tweets, vocab)\n",
    "print(f\"Model 6 topics: umass {umass}, cv {cv}, cuci {cuci}, cnpmi {cnpmi}\\n\")\n",
    "\n",
    "model7 = ProdLDA(vocab_size=docs.shape[1],num_topics=7,  hidden=100,dropout=0.2)\n",
    "model7.load_state_dict(torch.load(\"models_prodLDA/prodLDA_7_50.pth\"))\n",
    "umass,cv,cuci,cnpmi = compute_coherence(model7, 7, tweets, vocab)\n",
    "print(f\"Model 7 topics: umass {umass}, cv {cv}, cuci {cuci}, cnpmi {cnpmi}\\n\")\n",
    "\n",
    "model8 = ProdLDA(vocab_size=docs.shape[1],num_topics=8,  hidden=100,dropout=0.2)\n",
    "model8.load_state_dict(torch.load(\"models_prodLDA/prodLDA_8_50.pth\"))\n",
    "umass,cv,cuci,cnpmi = compute_coherence(model8, 8, tweets, vocab)\n",
    "print(f\"Model 8 topics: umass {umass}, cv {cv}, cuci {cuci}, cnpmi {cnpmi}\\n\")\n",
    "\n",
    "model9 = ProdLDA(vocab_size=docs.shape[1],num_topics=9,  hidden=100,dropout=0.2)\n",
    "model9.load_state_dict(torch.load(\"models_prodLDA/prodLDA_9_50.pth\"))\n",
    "umass,cv,cuci,cnpmi = compute_coherence(model9, 9, tweets, vocab)\n",
    "print(f\"Model 9 topics: umass {umass}, cv {cv}, cuci {cuci}, cnpmi {cnpmi}\\n\")\n",
    "\n",
    "model10 = ProdLDA(vocab_size=docs.shape[1],num_topics=10,  hidden=100,dropout=0.2)\n",
    "model10.load_state_dict(torch.load(\"models_prodLDA/prodLDA_10_50.pth\"))\n",
    "umass,cv,cuci,cnpmi = compute_coherence(model10, 10, tweets, vocab)\n",
    "print(f\"Model 10 topics: umass {umass}, cv {cv}, cuci {cuci}, cnpmi {cnpmi}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
