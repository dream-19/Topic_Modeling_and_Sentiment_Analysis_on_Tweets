{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Eseguo sentiment analysis sul dataset ottenuto di tweet, con anche le informazioni del topic di appartenenza.\n",
    "Utilizzo la libreria: tweetnlp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweetnlp #pip install tweetnlp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carico il documento\n",
    "source = './doc/results.csv'\n",
    "df = pd.read_csv(source) #campi: username,date,first_topic,first_topic_name,first_topic_prob,second_topic,second_topic_name,second_topic_prob,text,lemmatized_text,per_word_topics,per_word_topics_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fed869456f40f2bbd9ab834d1090b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1995dbec8fd40d2b5e18b14651ee4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/409 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2858ff4773b145728115c6521a63695e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/780k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6c9bafad8349498126bb854b22d770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f912cd68a7cd4363bbd23fa010dc9051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f617f9d7d904104a3c58f2d98dc1116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c282f9f2ef54ff593ae3d9e0d2bf60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load model\n",
    "model_sentiment = tweetnlp.load_model('sentiment') #positive, negative, neutral\n",
    "model_emotion = tweetnlp.load_model('emotion') # joy, anger, love, sadness, fear, surprise, optimism, pessimism, anticipation, disgust, trust\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have always worked toward common ground solutions that the American people and majority of Congress support. We can get something meaningful done if we do our jobs and come together. https://t.co/QuyByLu8y5\n",
      "{'label': 'positive', 'probability': {'negative': 0.00891607441008091, 'neutral': 0.12458395212888718, 'positive': 0.8664999008178711}}\n",
      "{'label': 'optimism', 'probability': {'anger': 0.000555713486392051, 'anticipation': 0.0063929022289812565, 'disgust': 0.00025548110716044903, 'fear': 0.0003591348649933934, 'joy': 0.18760699033737183, 'love': 0.003951615653932095, 'optimism': 0.7792908549308777, 'pessimism': 0.00030009658075869083, 'sadness': 0.00031694641802459955, 'surprise': 0.0006394168012775481, 'trust': 0.02033079043030739}}\n"
     ]
    }
   ],
   "source": [
    "#Testo la libreria\n",
    "print(df['text'][0])\n",
    "r = model_sentiment.sentiment(df['text'][0], return_probability=True)\n",
    "print(r)\n",
    "\n",
    "r2 = model_emotion.emotion(df['text'][0], return_probability=True)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New dataframe to save results\n",
    "destination = './doc/sentiment_analysis.csv'\n",
    "\n",
    "#Data\n",
    "data = {'username': [],\n",
    "        'date': [],\n",
    "        'first_topic': [],\n",
    "        'first_topic_name': [],\n",
    "        'first_topic_prob': [],\n",
    "        'sentiment': [],\n",
    "        'sentiment_prob': [],\n",
    "        'emotion': [],\n",
    "        'emotion_prob': [],\n",
    "        'second_topic': [],\n",
    "        'second_topic_name': [],\n",
    "        'second_topic_prob': [],\n",
    "        'text': [],\n",
    "        'lemmatized_text': [],\n",
    "        'per_word_topics': [],\n",
    "        'per_word_topics_name': [],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: sentiment analysis (positive, negative, neutral with probability) and emotion recognition\n",
    "for index, row in df.iterrows():\n",
    "    tweet = row['text']\n",
    "    \n",
    "    #sentiment\n",
    "    sentiment = model_sentiment.sentiment(tweet, return_probability=True)\n",
    "    label_sentiment = sentiment['label']\n",
    "    label_sentiment_prob = sentiment['probability'][label_sentiment]\n",
    "  \n",
    "    #Emotion\n",
    "    emotion = model_emotion.emotion(tweet, return_probability=True)\n",
    "    label_emotion = emotion['label']\n",
    "    label_emotion_prob = emotion['probability'][label_emotion]\n",
    "    \n",
    "    #add to data\n",
    "    data['username'].append(row['username'])\n",
    "    data['date'].append(row['date'])\n",
    "    data['first_topic'].append(row['first_topic'])\n",
    "    data['first_topic_name'].append(row['first_topic_name'])\n",
    "    data['first_topic_prob'].append(row['first_topic_prob'])\n",
    "    data['sentiment'].append(label_sentiment)\n",
    "    data['sentiment_prob'].append(label_sentiment_prob)\n",
    "    data['emotion'].append(label_emotion)\n",
    "    data['emotion_prob'].append(label_emotion_prob)\n",
    "    data['second_topic'].append(row['second_topic'])\n",
    "    data['second_topic_name'].append(row['second_topic_name'])\n",
    "    data['second_topic_prob'].append(row['second_topic_prob'])\n",
    "    data['text'].append(row['text'])\n",
    "    data['lemmatized_text'].append(row['lemmatized_text'])\n",
    "    data['per_word_topics'].append(row['per_word_topics'])\n",
    "    data['per_word_topics_name'].append(row['per_word_topics_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results\n",
    "df1 = pd.DataFrame(data)\n",
    "df1 = df1.sort_values(by='emotion')\n",
    "df1.to_csv(destination, index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
