{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#TUTORIAL: https://pyro.ai/examples/prodlda.html\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn #pip install torch torchvision torchaudio\n",
    "import torch.nn.functional as F\n",
    "import pyro #pip3 install pyro-ppl \n",
    "from pyro.infer import SVI, TraceMeanField_ELBO,  MCMC, NUTS \n",
    "import pyro.distributions as dist\n",
    "from tqdm import trange\n",
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Enable smoke test - run the notebook cells on CI.\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataset to be used in the model\n",
    "source = './doc/cleaned.csv'\n",
    "df = pd.read_csv(source)\n",
    "\n",
    "# Split the text data into words based on spaces\n",
    "tweets = df['lemmatized_text'].apply(lambda text: text.split())\n",
    "\n",
    "# Create a CountVectorizer \n",
    "# max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\n",
    "# min_df is used for removing terms that appear too infrequently, at least in 20 documents\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=20)\n",
    "\n",
    "# Convert the tokenized text data into a document-term matrix\n",
    "docs = torch.from_numpy(vectorizer.fit_transform([' '.join(tweet) for tweet in tweets]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 5590\n",
      "Corpus size: torch.Size([70000, 5590])\n"
     ]
    }
   ],
   "source": [
    "#Show the initial data\n",
    "vocab = pd.DataFrame(columns=['word', 'index'])\n",
    "vocab['word'] = vectorizer.get_feature_names_out()\n",
    "vocab['index'] = vocab.index\n",
    "print('Dictionary size: %d' % len(vocab)) #vocab_size\n",
    "print('Corpus size: {}'.format(docs.shape)) # (num_docs, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to execute prodLDAModel\n",
    "class Encoder(nn.Module):\n",
    "    # Base class for the encoder net, used in the guide\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fcmu = nn.Linear(hidden, num_topics)\n",
    "        self.fclv = nn.Linear(hidden, num_topics)\n",
    "        # NB: here we set `affine=False` to reduce the number of learning parameters\n",
    "        # See https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        # for the effect of this flag in BatchNorm1d\n",
    "        self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "        self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = F.softplus(self.fc1(inputs))\n",
    "        h = F.softplus(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        # Œº and Œ£ are the outputs\n",
    "        logtheta_loc = self.bnmu(self.fcmu(h))\n",
    "        logtheta_logvar = self.bnlv(self.fclv(h))\n",
    "        logtheta_scale = (0.5 * logtheta_logvar).exp()  # Enforces positivity\n",
    "        return logtheta_loc, logtheta_scale\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.drop(inputs)\n",
    "        # the output is œÉ(Œ≤Œ∏)\n",
    "        return F.softmax(self.bn(self.beta(inputs)), dim=1)\n",
    "\n",
    "\n",
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_topics = num_topics\n",
    "        self.encoder = Encoder(vocab_size, num_topics, hidden, dropout)\n",
    "        self.decoder = Decoder(vocab_size, num_topics, dropout)\n",
    "\n",
    "    def model(self, docs):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ùëù(ùúÉ|ùõº) is replaced by a logistic-normal distribution\n",
    "            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics))\n",
    "            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics))\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "            theta = F.softmax(logtheta, -1)\n",
    "\n",
    "            # conditional distribution of ùë§ùëõ is defined as\n",
    "            # ùë§ùëõ|ùõΩ,ùúÉ ~ Categorical(ùúé(ùõΩùúÉ))\n",
    "            count_param = self.decoder(theta)\n",
    "            # Currently, PyTorch Multinomial requires `total_count` to be homogeneous.\n",
    "            # Because the numbers of words across documents can vary,\n",
    "            # we will use the maximum count accross documents here.\n",
    "            # This does not affect the result because Multinomial.log_prob does\n",
    "            # not require `total_count` to evaluate the log probability.\n",
    "            total_count = int(docs.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs',\n",
    "                dist.Multinomial(total_count, count_param),\n",
    "                obs=docs\n",
    "            )\n",
    "\n",
    "    def guide(self, docs):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ùëù(ùúÉ|ùõº) is replaced by a logistic-normal distribution,\n",
    "            # where Œº and Œ£ are the encoder network outputs\n",
    "            logtheta_loc, logtheta_scale = self.encoder(docs)\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "\n",
    "    def beta(self):\n",
    "        # beta matrix elements are the weights of the FC layer on the decoder\n",
    "        return self.decoder.beta.weight.cpu().detach().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting global variables\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "pyro.set_rng_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "docs = docs.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# setting hyperparameters\n",
    "num_topics = 8\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "\n",
    "# training\n",
    "pyro.clear_param_store()\n",
    "\n",
    "prodLDA = ProdLDA(\n",
    "    vocab_size=docs.shape[1],\n",
    "    num_topics=num_topics,\n",
    "    hidden=100 if not smoke_test else 10,\n",
    "    dropout=0.2\n",
    ")\n",
    "prodLDA.to(device)\n",
    "\n",
    "optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "svi = SVI(prodLDA.model, prodLDA.guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "num_batches = int(math.ceil(docs.shape[0] / batch_size)) if not smoke_test else 1\n",
    "\n",
    "bar = trange(num_epochs)\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        batch_docs = docs[i * batch_size:(i + 1) * batch_size, :]\n",
    "        loss = svi.step(batch_docs)\n",
    "        running_loss += loss / batch_docs.size(0)\n",
    "\n",
    "    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
